{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will set up a basic RAG system using **Langchain** and Meta's **Llama** 3.2 1B model from Huggingface.\n",
    "\n",
    "We will use the **FAISS** vector database to store the document indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a.hastak\\anaconda3\\envs\\rag_explore\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Core dependencies\n",
    "import torch\n",
    "from torch import cuda\n",
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "import pickle\n",
    "from time import time\n",
    "\n",
    "# ML/NLP dependencies\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain \n",
    "from langchain.prompts import PromptTemplate \n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LLM models\n",
    "\n",
    "The **sentence transformer model** is used for generating vector embeddings from the documents. These vector emebeddings are used by the vector store when indexing documents and calculating the vector or semantic similarity between documents.\n",
    "\n",
    "The **Llama Instruct model** is used for text generation -- generating the response to the user query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ACCESS_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\") \n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'  \n",
    "MODEL_ID = 'meta-llama/Llama-3.2-1B-Instruct'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer():\n",
    "     \n",
    "    # Load tokenizer and model pipeline\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_ID, \n",
    "        token=ACCESS_TOKEN,  \n",
    "        device=device)   \n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = load_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(): \n",
    "    try:\n",
    "        accelerator = Accelerator(cpu=True if device == 'cpu' else False)  # Ensure we use the CPU with accelerate\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_ID, \n",
    "            device_map=device,\n",
    "            torch_dtype=torch.float32  # Ensure we load in float32 precision (default)\n",
    "        )\n",
    "        model = accelerator.prepare(model)  # Prepare the model for CPU execution\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = None\n",
    "# Embeddings and vector database setup\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"  \n",
    "model_kwargs = {\"device\": \"cuda\" if cuda.is_available() else \"cpu\"}  # Use GPU if available\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'vector_db'\n",
    "\n",
    "# Get the environment variable for the index folder\n",
    "index_path = os.getenv(\"INDEX_PATH\", default=os.path.join(os.getcwd(), '..', 'index', 'vector_db')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = FAISS.load_local(folder_path=index_path,index_name=index_name,embeddings=embeddings) #, allow_dangerous_deserialization = True)\n",
    "# Load the FAISS vector store (retriever_vectordb)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\":2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeeUnpacking Argument Listsfordetailsontheasteriskinthisline.\n",
      "5.2 Thedel statement\n",
      "Thereisawaytoremoveanitemfromalistgivenitsindexinsteadofitsvalue: the del statement. Thisdiffersfrom\n",
      "the~list.pop() method which returns a value. Thedel statement can also be used to remove slices from a list\n",
      "orcleartheentirelist(whichwedidearlierbyassignmentofanemptylisttotheslice). Forexample:\n",
      ">>> a = [-1, 1, 66.25, 333, 333, 1234.5]\n",
      ">>> del a[0]\n",
      ">>> a\n",
      "[1, 66.25, 333, 333, 1234.5]\n",
      ">>> del a[2:4]\n",
      ">>> a\n",
      "[1, 66.25, 1234.5]\n",
      ">>> del a[:]\n",
      ">>> a\n",
      "[]\n",
      "del canalsobeusedtodeleteentirevariables:\n",
      ">>> del a\n",
      "Referencing the namea hereafter is an error (at least until another value is assigned to it). Weâ€™ll find other uses for\n",
      "del later.\n",
      "5.2. Thedel statement 37\n",
      "{'source': '../data\\\\tutorial.pdf', 'page': 42}\n",
      "range.\n",
      "list.clear()\n",
      "Removeallitemsfromthelist. Similarto del a[:].\n",
      "list.index(x[,start[,end ]])\n",
      "Returnzero-basedindexinthelistofthefirstitemwhosevalueisequalto x. Raisesa ValueError ifthere\n",
      "isnosuchitem.\n",
      "The optional argumentsstart andend are interpreted as in the slice notation and are used to limit the search\n",
      "to a particular subsequence of the list. The returned index is computed relative to the beginning of the full\n",
      "sequenceratherthanthe start argument.\n",
      "list.count(x)\n",
      "Returnthenumberoftimes x appearsinthelist.\n",
      "list.sort(*,key=None,reverse=False)\n",
      "Sort the items of the list in place (the arguments can be used for sort customization, seesorted() for their\n",
      "explanation).\n",
      "list.reverse()\n",
      "Reversetheelementsofthelistinplace.\n",
      "33\n",
      "{'source': '../data\\\\tutorial.pdf', 'page': 38}\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"How to remove an item from a list?\")\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(doc.metadata) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template setup\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=\"\"\"\n",
    "    You are an python documentation assistant designed to provide a summarized, complete and holistic answer to the question using the information from the given context. \n",
    "    Do not provide general information or assumptions outside the given context. \n",
    "    If you dont know the answer, just say that you don't know.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=512, \n",
    "    top_k=5, \n",
    "    temperature=0.2,  \n",
    "    repetition_penalty=1.2,  \n",
    "    token=ACCESS_TOKEN,\n",
    "    eos_token_id=tokenizer.eos_token_id,  \n",
    "    early_stopping=False,  # Disable early stopping to allow complete responses\n",
    "    # device=0 if cuda.is_available() else -1,\n",
    "    )  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=query_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(question, chat_history):\n",
    "    start_time = time()\n",
    "    # Conversational retrieval chain setup\n",
    "    qa_1 = ConversationalRetrievalChain.from_llm(   \n",
    "        llm=llm,   \n",
    "        retriever = retriever,\n",
    "        return_source_documents=True, \n",
    "        combine_docs_chain_kwargs={\"prompt\": prompt_template}, \n",
    "        verbose = True,\n",
    "        ) \n",
    "    chain = qa_1({\"question\": question, 'chat_history': chat_history})\n",
    "    stop_time  = time()\n",
    "    print(f'Response generation took {stop_time - start_time:.2f} seconds.')\n",
    "    return chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how to remove an item from a list?\"\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are an python documentation assistant designed to provide a summarized, complete and holistic answer to the question using the information from the given context. \n",
      "    Do not provide general information or assumptions outside the given context. \n",
      "    If you dont know the answer, just say that you don't know.\n",
      "    Context: SeeUnpacking Argument Listsfordetailsontheasteriskinthisline.\n",
      "5.2 Thedel statement\n",
      "Thereisawaytoremoveanitemfromalistgivenitsindexinsteadofitsvalue: the del statement. Thisdiffersfrom\n",
      "the~list.pop() method which returns a value. Thedel statement can also be used to remove slices from a list\n",
      "orcleartheentirelist(whichwedidearlierbyassignmentofanemptylisttotheslice). Forexample:\n",
      ">>> a = [-1, 1, 66.25, 333, 333, 1234.5]\n",
      ">>> del a[0]\n",
      ">>> a\n",
      "[1, 66.25, 333, 333, 1234.5]\n",
      ">>> del a[2:4]\n",
      ">>> a\n",
      "[1, 66.25, 1234.5]\n",
      ">>> del a[:]\n",
      ">>> a\n",
      "[]\n",
      "del canalsobeusedtodeleteentirevariables:\n",
      ">>> del a\n",
      "Referencing the namea hereafter is an error (at least until another value is assigned to it). Weâ€™ll find other uses for\n",
      "del later.\n",
      "5.2. Thedel statement 37\n",
      "\n",
      "range.\n",
      "list.clear()\n",
      "Removeallitemsfromthelist. Similarto del a[:].\n",
      "list.index(x[,start[,end ]])\n",
      "Returnzero-basedindexinthelistofthefirstitemwhosevalueisequalto x. Raisesa ValueError ifthere\n",
      "isnosuchitem.\n",
      "The optional argumentsstart andend are interpreted as in the slice notation and are used to limit the search\n",
      "to a particular subsequence of the list. The returned index is computed relative to the beginning of the full\n",
      "sequenceratherthanthe start argument.\n",
      "list.count(x)\n",
      "Returnthenumberoftimes x appearsinthelist.\n",
      "list.sort(*,key=None,reverse=False)\n",
      "Sort the items of the list in place (the arguments can be used for sort customization, seesorted() for their\n",
      "explanation).\n",
      "list.reverse()\n",
      "Reversetheelementsofthelistinplace.\n",
      "33\n",
      "    Question: how to remove an item from a list?\n",
      "    \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response generation took 67.87 seconds.\n"
     ]
    }
   ],
   "source": [
    "chain = get_llm_response(question,chat_history) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Answer: use the `del` statement.\n",
      "\n",
      "### Example Use Case:\n",
      "\n",
      "```python\n",
      "my_list = [1, 2, 3, 4, 5]\n",
      "\n",
      "# Remove first element by its position\n",
      "print(my_list.remove(1)) # Output: 2\n",
      "```\n",
      "\n",
      "In this example, we're removing the first element at index 0 because there's no such thing called \"remove\" function but instead we have `remove()` method provided by Python List class. \n",
      "\n",
      "Note - In some cases like when trying to delete multiple elements with same index then only one will get deleted while others remain intact. Also note that deleting an element doesnâ€™t change any existing indices so they still point to original values before deletion.\n"
     ]
    }
   ],
   "source": [
    "print(chain[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_explore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
